Suppose I am deliberating whether I should live on a boat and sail the Caribbean for a year. This is a decision not to be taken lightly. Many factors will matter for my decision. Several of these depend on uncertain states of the world. Will I be able to make a living? Is my boat really seaworthy? Will I miss my friends? How bad will the next winter be in my home town?


\section{Decision Problems and the Uses of Decision Theory}\label{s1}

%\subsection{A decision problem}\label{subs11}

Giving a decision problem like this some formal structure may be helpful for a number of interrelated purposes. As an agent, it might help me come to a better decision. But giving formal structure to a decision problem may also help a third party: prior to an action, it may help them predict my behaviour. And after the action, it may help them both understand my action, and judge whether I was rational. Moreover, giving formal structure to a decision problem is a pre-requisite for applying formal decision theories. And formal decision theories are used for all the aforementioned purposes.

In the case of the decision whether to live on a boat, we could perhaps represent the decision problem as shown in Table \ref{t1}. In this matrix, the rows represent the actions I might take. In our case, these are to either live on a boat, or not to live on a boat. The columns represent the relevant states of the world. These are conditions that are out of my control, but matter for what I should do. Suppose these involve my boat either being seaworthy, or not being seaworthy. I am uncertain which of these states of affairs will come about. Finally, the entries in the matrix describe the possible outcomes I care about that would result from my action combined with a state of the world.

\FloatBarrier
\begin{table}
\centering
\begin{tabularx}{\textwidth}{ X|X|X| } \cline{2-3} & Boat seaworthy & Boat not seaworthy \\ \cline{1-3} \multicolumn{1}{|l|} {Live on a boat} & Life on a boat \newline no storm damage & Life on a boat \newline storm damage \\ \hline \multicolumn{1}{|l|} {Stay in home town} & Life as usual & Life as usual \\ \hline \end{tabularx}
\caption{Should I live on a boat?}
\label{t1}
\end{table}

Since Savage's (1954) decision theory, it has become standard to characterise decision problems with state-outcome matrices like the one I just introduced. More generally, let $A_{1} \ldots A_{n}$ be a set of $n$ actions that are open to the agent, and let $S_{1} \ldots S_{m}$ be $m$ mutually exclusive and exhaustive states of the world. These actions and states of the world combine to yield a set of $n \cdot m$ outcomes $O_{11} \ldots O_{nm}$. Table \ref{t2} shows this more general state-outcome matrix.

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l|l|l}
\cline{2-4}
                          & $S_{1}$  & ... & $S_{m}$  &  \\ \cline{1-4}
\multicolumn{1}{|l|}{$A_{1}$}  &$ O_{11}$ & ... & $O_{1m}$ &   \\ \cline{1-4}
\multicolumn{1}{|l|}{...} & ... & ... & ... &   \\ \cline{1-4}
\multicolumn{1}{|l|}{$A_{n}$}  & $O_{n1}$ & ... & $O_{nm}$ &   \\ \cline{1-4}
\end{tabular}
\caption{State-outcome matrix}
\label{t2}
\end{table}

Given such a representation of a decision problem, formal decision theories assume that agents have various attitudes to the elements of the state-outcome matrix. Agents are assumed to have preferences over the outcomes their actions might lead to. Depending on our interpretation of decision theory, we may also assume that agents can assign a utility value to the outcomes, and a probability value to the states of the world. Decision theories then require the preferences the agent has over actions, which are assumed to guide her choice behaviour, to relate to those other attitudes in a particular way.

\subsection{Expected utility maximisation}\label{subs12}

Traditionally, the requirement that decision theories place on agents under conditions of uncertainty has been that agents should maximise their expected utility, or act as if they did. Decision theories which incorporate this requirement are known under the heading of `expected utility theory'. In the special case where an agent is certain about the consequences of each of her actions, this requirement reduces to the requirement to maximise utility. Since we are always to some extent uncertain about the consequences of our actions, I will focus on the uncertain case here.\footnote{I understand decision-making under `uncertainty' here to refer to any case where an agent is not certain what the consequences of her actions will be, or what state will come about. A distinction is sometimes made between risk, uncertainty, ignorance and ambiguity, where `risk' refers to the case where objective probabilities are known, `uncertainty' refers to the case where an agent can make a subjective judgement about probabilities, an agent is in a state of `ignorance' if she cannot make such probability assignments, and `ambiguity' occurs when an agent can make probability assignments for some states, but not others. While these differences will play a role later in this entry, it is not helpful to make these distinctions at this point.} However, much of the following discussion will also apply to decision-making under certainty. Moreover, most of this entry will focus on expected utility theory. Some alternative decision theories are discussed in Section \ref{s6}.

As we will see, the requirement to maximise expected utility takes different forms under different interpretations of expected utility theory. For now, let us assume that agents can assign utility values $u(O)$ to outcomes, and probability values $p(S)$ to states of the world. The expected utility is then calculated by weighting the utility of each possible outcome of the action by the probability that it occurs, and summing them together. Expected utility theory instructs us to prefer acts with higher expected utility to acts with lower expected utility, and to choose one of the acts with the highest expected utility.

In our example, suppose that I think that the chances that my boat is seaworthy are 50\%, and that the relevant utilities are the ones given in Table \ref{t3}. In that case, the expected utility of living on a boat will be $0.5 \cdot 200 + 0.5 \cdot 20 = 110$, while the expected utility of staying in my home town is $100$. I conclude I should live on a boat.

\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\cline{2-4}
\multicolumn{1}{l|}{}                   & \multicolumn{1}{l|}{Boat seaworthy} & \multicolumn{1}{l|}{Boat not seaworthy} & \multicolumn{1}{l|}{\textbf{EU}} &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Live on a boat}    & \multicolumn{1}{l|}{200}               & \multicolumn{1}{l|}{20}                    & \multicolumn{1}{l|}{\textbf{110}}              &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Stay in home town} & \multicolumn{1}{l|}{100}               & \multicolumn{1}{l|}{100}                   & \multicolumn{1}{l|}{\textbf{100}}              &  \\ \cline{1-4}
\end{tabular}
\caption{Decision problem with utilities}
\label{t3}
\end{table}

Formally, the expected utility $EU(A)$ of an action can be expressed as follows:
$$EU(A_{i}) = \sum\limits_{j=1}^m p (S_j) \cdot u (O_{ij}).$$
Expected utility theory requires agents to prefer acts for which this weighted sum is higher to acts for which this weighted sum is lower, and to choose an action for which this weighted sum is maximised.

\subsection{The uses of decision theory}\label{subs13}

Now we can see how expected utility theory could be put to each of the different uses mentioned above. The requirement to maximise expected utility (or to act as if one did), however it is understood, is considered as a requirement of practical rationality by proponents of expected utility theory. In particular, the requirements of expected utility theory are often interpreted to capture what it means to be instrumentally rational, that is, what it means to take the appropriate means to one's ends, whatever those ends may be. We will see how this may be cashed out in more detail in Section \ref{s3}, when we discuss different interpretations of expected utility theory. For now, note that if we take the utility function to express the agent's ends, then the requirement to maximise the expectation of utility sounds like a natural requirement of instrumental rationality.

Sometimes, the requirements of expected utility theory are also understood as expressing what it means to have coherent ends in the first place. Constructivists about utility (see Section \ref{subs31}) often understand expected utility theory as expressing requirements on the coherence of preferences. But on that understanding, too, expected utility theory does not make any prescriptions on the specific content of an agent's ends. It merely rules out certain combinations of preferences. And so for those who think that some ends are irrational in themselves, expected utility theory will at best be an incomplete theory of practical rationality.

If we understand the requirements of expected utility theory as requirements of practical rationality, it seems like expected utility theory could help me as an agent make better decisions. After I have formally represented my decision problem, expected utility theory could be understood as telling me to maximise my expected utility (or to act as if I did). In the above example, we employed expected utility theory in this way. Expected utility theory helped me decide that I should live on a boat. In this guise, expected utility theory is an {\em action-guiding} theory.

From a third party perspective, expected utility theory could also be used to judge whether an agent's action was rational. Having represented the agent's decision problem formally, we judge an action to be rational if it was an act with maximum expected utility. This understands expected utility theory as a {\em normative} theory: a theory about what makes it the case that somebody acted rationally.

It is important to note the difference between the action-guiding and the normative uses of expected utility theory.\footnote{Herbert Simon famously drew attention to this difference when he distinguished between {\em procedural} and {\em substantive} rationality, drawing on a similar distinction made by \citet{Weber1922}. See \citet{Simon1976}.} An action can be rational according to normative expected utility theory even if the agent did not use expected utility theory as an action-guiding theory. One could even hold that expected utility theory is a good normative theory while being a bad action-guiding theory. This would be the case if most agents are bad at determining their expected utility, and do better by using simpler heuristics.\footnote{Starting with \citet{TverskyKahneman1974}, there has been a wealth of empirical literature studying what kind of heuristics decision-makers use when making decisions under uncertainty, and how well they perform. See, for instance, \citet{Payneetal1993} and \citet{GigerenzerTodd2000}.}

Expected utility theory is also often put to an explanatory or predictive use, especially within economics or psychology. If we assume that agents follow the requirements of expected utility theory, and we know enough of their preferences or utility and probability assignments, we can use the theory to predict their behaviour. In this context, philosophers have been interested more in whether decision theory can help us {\em understand} an agent's actions. Interpreting an agent as maximising her expected utility in a formal decision problem may reveal her motives in action, and thus explain her action.

In fact, there is a tradition in the philosophy of action that claims that explaining another's behaviour always involves rationalising her behaviour to some extent. \citet{Davidson1973} introduced the label `radical interpretation' for the attempt to infer an agent's attitudes, such as her beliefs and desires, from her actions. He believed that this was only possible if we assume certain rationality constraints on how these attitudes relate. \citet{Ramsey1926} had already used expected utility theory to infer an agent's probabilities, and thus, he argued, her beliefs from her behaviour. \citet{Lewis1974} showed that expected utility theory captures Davidson's constraints on the relationship between beliefs and desires, and thus can be used to elicit beliefs and desires. Davidson himself later argued, in \citet{Davidson1985}, that expected utility theory can be extended to further elicit an agent's {\em meanings}, that is, her interpretation of sentences. This is sometimes known as the {\em interpretive} use of decision theory.

And so in the philosophical literature, expected utility theory has been used as an action-guiding theory, a normative theory, and an interpretive theory.\footnote{\citet{Bermudez2009} draws a similar tri-partite distinction between the normative, action-guiding and explanatory/predictive dimensions of decision theory. Similarly, \citet{Buchak2016} distinguishes between the normative and interpretive uses of decision theory.} Other decision theories have been put to the same uses. As we will see in Section \ref{s6}, there are alternatives to expected utility theory that offer rival prescriptions of practical rationality. However, most alternatives to expected utility theory have been introduced as primarily descriptive theories, that are used to predict and explain behaviour that need not be rational.

Now that we have seen what kinds of uses expected utility theory can be put to, the next section will look at some influential applications of expected utility theory.

\subsection{Some Applications}\label{subs131}

Expected utility theory has proven to be an enormously fruitful theory, that has been applied in various different fields and disciplines. Originally, it found application mostly in the theory of consumer choice. This field of economics studies why consumers choose some goods rather than others, and helps to predict market outcomes. Expected utility theory has been used to explain the shape of demand curves for goods. The demand for insurance, in particular, is difficult to understand without a formal theory of choice under uncertainty. Expected utility theory has also helped to explain some phenomena that had previously seemed surprising. A classic example here is adverse selection, which occurs when there is an information asymmetry between buyers and sellers in the market. In these kinds of situations, sellers of high quality goods may be driven out of the market. \citet{Akerlof1970} first explained this phenomenon, and a rich literature has developed since. \citet{EinavFinkelstein2011} provide a helpful overview of work on adverse selection in insurance markets.

Decision theory has also found application in many fields outside of economics. For instance, in politics, it has been used to study voting and voter turn-out,\footnote{\citet{Downs1957} counts as the first systematic application of decision theoretic models from economics to politics. For recent work on voting specifically, see \citet{Feddersen2004}.} in law it has been used to study judicial decisions,\footnote{See, for instance, \citet{EpsteinLandesPosner2013}.} and in sociology it has been used to explain class and gender differences in levels of education.\footnote{See, for instance, \citet{BreenGoldthorpe1997}.}

Expected utility theory has also been influential in philosophy. Apart from it being an important contender as a theory of practical rationality, expected utility theory plays an important role in ethics, in particular in consequentialist ethics. Along with \citet{Jackson1991}, many consequentialists believe that agents ought to maximise expected moral goodness. Moreover, expected utility theory has been applied to the question of what agents ought to do in the face of moral uncertainty---uncertainty about what one ought to do, or even about which moral theory is the right one.\footnote{See, for instance, \citet{Lockhart2000}, and \citet{Sepielli2013} for a criticism of Lockhart's approach.}

Recently, expected utility theory has found application in epistemology in the form of {\em epistemic decision theory}. Here, agents are modeled as receiving epistemic utility from being in various epistemic states, such as being certain of the proposition that my boat is sea-worthy. I will receive a high epistemic utility from being in that state in the case where my boat in fact turns out to be seaworthy, and low epistemic utility when my boat turns out not to be seaworthy. Agents are then modeled as maximising their expected epistemic utility. Epistemic utility theory has been used to justify various epistemic norms, such as probabilism (the norm that an agent's credences should obey the probability calculus), and conditionalisation (the norm that agents should update their credences by conditionalizing their old credence on the new evidence they received). For an overview of these arguments, see \citet{Pettigrew2011}.

\subsection{Formulating decision problems}\label{subs14}

How should the decision problems that formal decision theories deal with be formulated in the first place? In order to apply a formal decision theory, the choices an agent faces need to already be represented as a formal decision problem. Table \ref{t1} offered one representation of my choice of whether to live on a boat. But how can we be sure it was the right one?

For his decision theory, \citet{Savage1954} assumed that states are descriptions of the world that include everything that might be relevant to the agent. Similarly, he thought that descriptions of outcomes are descriptions of ``everything that might happen to the person'' (p. 13). Joyce (1999, p. 52) cashes out a rule for specifying outcomes that also appeals to relevance. He claims that a description of an outcome should include everything that might be relevant to the agent, in the following sense: whenever there is some circumstance such that an agent would strictly prefer an outcome in the presence of that circumstance to the same outcome in the absence of that circumstance, the outcome has been underspecified. Importantly, this implies that an agent's evaluation of an outcome should be independent of the state it occurs in, and the act that brought it about. All of this means that the sets of states and outcomes will end up being very fine-grained. Moreover, Savage also thinks of actions as functions from states to outcomes. This means that in each state, each action leads to a unique outcome. To ensure this, the set of actions, too, will have to be very fine-grained.

Note that this means that the decision problem I presented in Table \ref{t1} was hopelessly underspecified. When it comes to the decision of whether to live on a boat for a year or not, I do not only care about whether my boat will have storm damage or not. I also care, for instance, about whether I will have enough money for the year. I will evaluate the outcome ``Life on a boat, no storm damage'' differently depending on whether I will have enough money for the year or not. In fact, the exact amount of money I will have is going to matter for my decision. And so my decision problem should really distinguish between many different states of affairs involving me having more or less money, and the many different outcomes that occur in these states of affairs.

\citet{Jeffrey1983}, who offered a famous alternative to Savage's decision theory (see Section \ref{subs24}), and treated states, acts, and outcomes all as propositions, went so far as to define outcomes such that they entail an act and a state. An act and a state are also supposed to entail the outcome, and so we can simply replace outcomes with the conjunction of an act and a state in the decision matrix.

These ways of individuating outcomes will obviously lead to very large decision matrices for any real life decision. There are two reasons why we might find this problematic. The first reason has to do with the efficiency of the decision-making process. If we want our decision theory to be an action-guiding theory, then decision problems can't be so complex that ordinary agents cannot solve them. An action-guiding theory should be efficient in its application. Efficiency may also be a concern for the interpretive project. After all, this project wants to enable us to interpret each other's actions. And so doing so should not be overly complicated.

Savage called decision problems that specify every eventuality that might be relevant to an agent's choice ``grand world'' decision problems. \citet{Joyce1999} holds that we should really be trying to solve such a grand-world problem, but acknowledges that real agents will always fall short of this. Instead, he claims, they solve ``small world'' decision problems, which are coarsenings of grand-world decision problems. If we treat acts, states and outcomes as propositions, this means that the acts, states and outcomes of the small world decision problems are disjunctions of the acts, states, and outcomes of the grand-world decision problem. The decision problem described in Table \ref{t1} is such a small-world decision problem.

Joyce (1999, p. 74) holds that an agent is rational in using such small-world decision problems to the extent that she is justified in believing that her solution to the small-world decision problem will be the same as her solution to the grand-world decision problem would be. This permits the use of small world decision problems both for the action-guiding and normative purposes of decision theory whenever the agent is justified in believing that they are good enough models of the grand-world decision problem.

Joyce argues that this condition is met in Jeffrey's decision theory {\em if} an agent correctly evaluates all coarse outcomes and actions, while it is not generally met in Savage's decision theory. As will be explained in Section \ref{subs24}, this is due to the feature of {\em partition invariance}, which Jeffrey's theory has and Savage's theory does not. Despite these arguments, if efficiency in decision-making is an important concern, as it is for an action-guiding theory, one might think that an agent should sometimes base her decision on a small-world decision problem even if she is fairly certain that her decision based on the grand-world decision problem will be different. She might think that her solution to a small-world decision problem will be close enough to that of the grand-world decision problem, while solving the small-world decision problem will save her costs of deliberation.

The second argument against having too fine-grained a decision problem is that this makes expected utility theory not restrictive enough. As will be explained in more detail in Section \ref{s2}, the axioms used in the representation theorems of expected utility theory concern what combination of preferences are permissible. If preferences attach to outcomes, and outcomes can be individuated as finely as we like, then the danger is that the norm to abide by the axioms of decision theory does not constrain our actions much.

For instance, consider the following preference cycle, where $a$, $b$ and $c$ are outcomes, and $\prec$ expresses strict preference:
$$a \prec b \prec c \prec a.$$
Preference cycles such as this are ruled out by the transitivity axiom, which all representation theorems we shall look at in Section \ref{s2} share. When outcomes can be individuated very finely, the following two problems may arise. Firstly, a number of authors have worried that any potential circularity in an agent's preferences can be removed by individuating outcomes more finely, such that there is no circularity anymore. Secondly, and relatedly, fine individuation may mean that no outcome can ever be repeated. In that case, an agent cannot reveal a preference cycle in her actions, and so we cannot interpret her as being irrational.

To see this, note that if we treat the first and the second occurrence of outcome $a$ above as two different outcomes, say $a_1$ and $a_2$, the circularity is removed:
$$a_1 \prec b \prec c \prec a_2.$$
The worry is that this can always be done, for instance by distinguishing ``option a if it is compared to b'' from ``option a if it is compared to c''. If this strategy is always available, in what sense is the transitivity axiom a true restriction of the agent's preferences and actions? If we can't show that decision theory puts real restrictions on an agent's choices, then this is a problem especially for the action-guiding and normative projects.

A number of authors\footnote{See, especially, \citet{Broome1991}, \citet{Pettit1991} and \citet{Dreier1996}.} have held that this problem shows that the axioms of decision theory on their own cannot serve as a theory of practical rationality (even a partial one), but have to be supplemented with a further principle in order to serve their function. Broome (1991, chapter 5) notes that the problem can be dealt with by introducing rational requirements of indifference. Rational requirements of indifference hold between outcomes that are modeled as different, but that it would be irrational for the agent to have a strict preference between. If there was a rational requirement of indifference between $a_1$ and $a_2$, for instance, the preference cycle would be preserved.

However, we may also restrict how finely outcomes can be individuated to solve the problem, by not allowing a distinction between $a_1$ and $a_2$. Broome (1991, chapter 5) advocates a rule of individuation by justifiers that serves the same role as the rational requirements of indifference. According to this rule, two outcomes can only be modeled as distinct if it is not irrational to have a strict preference between them.

\citet{Pettit1991} proposes an alternative rule for individuation: two outcomes should be modeled as distinct just in case they differ in some quality the agent cares about, where caring about a quality cannot itself be cashed out in terms of preferences over outcomes. And \citet{Dreier1996} argues that two outcomes should be distinguished just in case there are circumstances where an agent has an actual strict preference between them. Note that this rule for individuation is equivalent to the one proposed by Joyce, but Pettit's and Broome's rules may lead to coarser grained individuations of decision problems. The coarser grained the individuations, the more restrictive the axioms of expected utility theory end up being.

\section{Representation Theorems} \label{s2}
%2: I think it would be good to illustrate how representation theorems do their magic. As it is, it's kind of mysterious how one might be able to derive a unique, quantitative representation from qualitative information about preferences. Readers may not have a very clear or concrete sense of what the preference axioms require, hindering their ability to appreciate the ensuing discussion. A concrete illustration could help make things more definite. E.g., something like Jeffrey's illustration of the vNM/Ramsey techniques using choices between kinds of sandwiches: ham/egg/tuna/etc. (chapter 3 of TLoD).

\subsection{The preference relation}\label{subs21}

In decision theory, representation theorems are proofs that an agent's preferences are representable by a function that is maximised by the agent. In the case of expected utility theory, they are proofs that an agent's preferences are such that we can represent her as maximising an expected utility function. As we will see in Section \ref{s3}, many decision theorists believe that utility is nothing more than a convenient way to represent preferences. Representation theorems are crucial for this interpretation of utility. The significance of the representation theorems will be further discussed in Section \ref{subs32}.

A weak preference relation is a binary relation $\succcurlyeq$, which is usually interpreted either as an agent's disposition to choose, or her judgements of greater choiceworthiness.\footnote{Many economists interpret preference as `revealed preference', and claim that an agent counts as preferring $x$ to $y$ just in case she actually chose $x$ when $y$ was also available. Such pure behaviourism is usually rejected in the philosophical literature because it takes away from the explanatory power of preferences, and does not allow for counter-preferential choice. For a critique of the notion of revealed preference, see \citet{Hausman2000}.} An agent weakly prefers $x$ to $y$ if she finds $x$ at least as choiceworthy as $y$, or if she is disposed to choose $x$ when $x$ and $y$ are available.

We can also define an indifference relation $\sim$ and a strict preference relation $\succ$ in terms of the weak preference relation $\succcurlyeq$:
\begin{enumerate}
\item $x \sim y$ if and only if $x \succcurlyeq y$ and $y \succcurlyeq x$.
\item $x \succ y$ if and only if $x \succcurlyeq y$ and not $y \succcurlyeq x$.
\end{enumerate}
Representation theorems take such preference relations as their starting point. They then proceed by formulating various axioms that pose restrictions on the preference relation, some of which are interpreted as conditions of rationality. Let $X$ be the domain of the preference relation. What representation theorems prove is the following. If an agent's preferences conform to the axioms, there will be a probability function and a utility function such that:
$$\textrm{For all $x$ and $y \in X$, } EU(x) \geq EU(y) \textrm{ if and only if } x \succcurlyeq y.$$

All the representation theorems described in the following assume that the preference relation is a {\em weak ordering} of the elements in its domain. That means that the preference relation is transitive and complete:
\begin{description}
\item[Transitivity:] For all $x, y$ and $z \in X, x \succcurlyeq y$ and $y \succcurlyeq z$ implies that $x \succcurlyeq z$.
\item[Completeness:] For all $x$ and $y \in X, x \succcurlyeq y$ or $y \succcurlyeq x$.
\end{description}
Section \ref{s4} will discuss potential problems with both completeness and transitivity.

Different representation theorems differ both in terms of the domain over which the preference relation is defined, and in terms of the other axioms needed for the representation theorem. They also differ in how many of the agent's attitudes other than preferences they take for granted. Consequently, they result in representation theorems of different strength.

\subsection{Von Neumann and Morgenstern}\label{subs22}

One of the first representation theorems for expected utility is due to \citet{vNM1944} and takes probabilities for granted.\footnote{An earlier representation theorem is due to \citet{Ramsey1926} and derives probabilities as well as utilities. It is often considered as a precursor to Savage's and Bolker's representation theorems, discussed below. See \citet{Bradley2004}.} In this representation theorem, the objects of preference are {\em lotteries}, which are either probability distributions $L = (p_1, ..., p_m)$ over the $m$ outcomes, or probability distributions over these `simple' lotteries. Probabilities are thus already part of the agent's object of preference.

While it helps to think of lotteries in the ordinary sense of monetary gambles where there is a known probability of winning some prize, von Neumann and Morgenstern intended for their representation theorem to have wider application. In our original example, if there is a $50\%$ chance that my boat is seaworthy, then I face a $50/50$ lottery over the outcomes described in Table \ref{t1}. Note furthermore that, since we are dealing directly with probability distributions over outcomes, there is no need to speak of states of the world.

While von Neumann and Morgenstern's representation theorem is perhaps most naturally understood given an objective interpretation of probability, their representation theorem is in fact compatible with any interpretation of probability. All we need is to already have access to the relevant (precise) probabilities when applying the representation theorems. If we think of probability as the agent's subjective degrees of belief, we already need to know what those subjective degrees of belief are. If we think of it as objective chance, we need to already know what those objective chances are.

What von Neumann and Morgenstern go on to prove in their representation theorem is that, provided an agent's preferences over lotteries abide by certain axioms, there is a utility function over outcomes such that an agent prefers one lottery over another just in case its expected utility is higher. One crucial axiom needed for this representation theorem is the independence axiom, discussed in Section \ref{subs51}.

Note that the result is not that there is one unique utility function which represents the agent's preferences. In fact, there is a family of utility functions which describe the agent's preferences. According to von Neumann and Morgenstern's representation theorem, any utility function which forms part of an expected utility representation of an agent's preferences will only be unique up to positive, linear transformations. The different utility functions that represent an agent's preferences will thus not all share the same zero point. What outcome will yield twice as much utility will then also differ between different utility functions. It is therefore often claimed that these properties of utility functions represent nothing ``real''. What is invariant between all the different utility functions that represent the agent's preferences, however, are the ratios of utility differences, which can capture the curvature of the utility function. Such ratios are often used to measure an agent's level of risk aversion.\footnote{Risk aversion is further discussed in Section \ref{subs53}. Also see \citet{Mas-Colell1995}, chapter 6 for more detail on expected utility theory's treatment of risk aversion.}

\subsection{Savage}\label{subs23}

While von Neumann and Morgenstern's representation theorem provides a representation of an agent's preferences where probabilities are already given, \citet{Savage1954} infers both a utility function and probabilities from an agent's preferences.\footnote{This is why von Neumann and Morgenstern's theory is sometimes referred to as a theory of decision-making under risk, and Savage's is referred to as a theory of decision-making under uncertainty. In the former, probabilities are already known, in the latter, subjective probabilities can be assigned by the agent. However, note that, as we pointed out above, von Neumann and Morgenstern's theory can also be applied when probabilities are subjective.} As we have already seen, the standard tripartite distinction of actions, outcomes and states of the world goes back to Savage. Instead of assuming, like von Neumann and Morgenstern did, that we can assign probabilities to outcomes directly, we introduce a set of states of the world, which determine what outcome an act will lead to. The agent does not know which of the states of the world will come about.

Savage takes the agent's preferences over acts as input, and introduces a number of axioms on these preferences. He derives both a probability function over states, which abides by the standard axioms of probability, and a utility function over outcomes which, like the one von Neumann and Morgenstern derived, is unique up to positive linear transformations. Together, they describe an expected utility function such that an act is preferred to another just in case it has a higher expected utility. Importantly, the agents in Savage's decision theory abide by the sure-thing principle, which serves a role similar to the independence axiom in von Neumann and Morgenstern's representation theorem, and will also be discussed in Section \ref{subs51}.

Acts, states and outcomes are all treated as theoretical primitives in Savage's framework. But Savage's representation theorem relies on a number of controversial assumptions about the act, state and outcome spaces and their relation. For one, probabilities apply only to states of the world, and utilities apply only to outcomes. Preferences range over both acts and outcomes. Savage assumed that an act and a state together determine an outcome. Most controversially, Savage assumes that there are what he calls {\em constant acts} for each possible outcome, that is, acts which bring about that outcome in any state of the world. For instance,  there must be an act which causes me great happiness even in the event that the apocalypse happens tomorrow. What makes things worse, by completeness, agents are required to have preferences over all these acts. \citet{LuceSuppes1965} take issue with Savage's theory for this reason.

While the results of Savage's representation theorem are strong, they rely on these strong assumptions about the structure of the act space. This is one reason why many decision theorists prefer Jeffrey's decision theory and Joyce's modification thereof.

\subsection{Jeffrey, Bolker and Joyce}\label{subs24}

Jeffrey's decision theory, developed in \citet{Jeffrey1983}, uses an axiomatisation by \citet{Bolker1966}. While he does not rely on an act space as rich as Savage's, Jeffrey preserves the tripartite distinction of acts, states and outcomes. However, for him, all of these are propositions, which means he can employ the tools of propositional logic. Moreover, preferences, utility and probability all range over all three. Agents end up assigning probabilities to their own acts,\footnote{This is a controversial feature of the theory. See \citet{Spohn1977} for criticism of this assumption.} and assigning utilities to states of the world.

Jeffrey's theory is sometimes known as conditional expected utility theory, because agents who follow the axioms of his decision theory are represented as maximisers of a conditional expected utility. In Savage's decision theory, the utilities of outcomes are weighted by the unconditional probability of the states in which they occur. This is also the formulation we presented in Section \ref{subs12}. In the example there, we weighted the possible outcomes by the probability of the state they occur in. For instance, we weighted the outcome of enjoying a year on a boat without damages by the probability of my boat being seaworthy.

Jeffrey noted that the unconditional nature of Savage's decision theory may produce the wrong results in cases where states are made more or less likely by performing an action. In our example, suppose that, for whatever reason, my choosing to live on a boat for a year makes it more likely that my boat is seaworthy. The unconditional probability of the boat being seaworthy is lower than the probability of it being seaworthy given I decide to live on the boat. And thus using the unconditional probability may lead to the judgement that I shouldn't spend the year on the boat, because the probability of it not being seaworthy is too high---even if the boat will be very likely to be seaworthy if I choose to do so. To avoid this problem, Jeffrey argued, it is better to use probabilities that are in some sense conditional on the action whose expected utility we are evaluating. We should weight the outcome of spending a year on a boat without damage by the probability of the boat being seaworthy given that I choose to live on the boat for a year.\footnote{Savage's own solution to the problem is that, for his formalism to apply, states and acts need to be specified such that there is no dependence between an action being performed and the likelihood of a state. Jeffrey's response is more elegant in that it requires no such restriction on what kinds of decision problems it can be applied to.}

Let the probability of a state given an act be $p_{A} (S)$. There is much disagreement on how this probability is to be interpreted. The main disagreement is whether it should be given a causal or an evidential interpretation. I postpone this discussion to Section \ref{subs33}. But let me note here that Jeffrey himself falls on the evidential side. Conditional expected utility theory advises us to maximise the following:
$$EU(A_{i}) = \sum\limits_{j=1}^m p_{A_i} (S_j) \cdot u (O_{ij})$$
Jeffrey interprets this conditional expected utility as an act's `news value', that is, as measuring how much an agent would appreciate the news that the act is performed.

The conditional nature of Jeffrey's decision theory is also what leads to its partition invariance.\footnote{See \citet{Joyce1999}, pp. 121-122.} In Jeffrey's theory, the value of a disjunction is always a function of the value of its disjuncts. For instance, the value of a coarse outcome $O_{1-10}$ which is a disjunction of outcomes $O_1,\ldots,O_{10}$ is a function of the values of the outcomes $O_1,\ldots, O_{10}$. But we could also subdivide the coarse outcome $O_{1-10}$ differently. $O_{1-10}$ is also a disjunction of the coarse outcomes $O_{1-5}$ and $O_{6-10}$, which are themselves disjunctions of $O_1,\ldots,O_5$ and $O_6,\ldots,O_{10}$ respectively. And so we can also calculate the value of $O_{1-10}$ from the values of $O_{1-5}$ and $O_{6-10}$. Partition invariance means that we get the same value in either case. The value of $O_{1-10}$ can be represented as a function of the values of any of its subdivisions. This means that, as long as utilities are assigned correctly to disjunctions, Jeffrey's decision theory gives equivalent recommendations no matter how finely we individuate outcomes, states and actions. Joyce argues that for this reason, the use of small-world decision problems is legitimate in Jeffrey's decision theory (see Section \ref{subs14}), and that that is a major advantage over Savage's unconditional, and partition variant decision theory.

Jeffrey's and Bolker's representation theorem is less strong than Savage's. It does not pin down a unique probability function. Nor does it result in a utility function that is unique up to positive linear transformations. Instead, it only ensures that probability and utility pairs are unique up to fractional linear transformations.\footnote{A fractional linear transformation transforms $u$ to $\frac{a \cdot u + b}{c \cdot u + d}$, with $a \cdot d - b \cdot c > 0$.}

\citet{Joyce1999} argues that this shows that we need to augment Jeffrey's and Bolker's representation theorem with assumptions about belief, and not merely preference. Unlike von Neumann and Morgenstern, however, he does not propose to simply assume probabilities. Instead, he introduces a `more likely than' relation, on which we can formulate a number of axioms, just as we did for the preference relation. The resulting representation theorem results in a unique probability function and a utility function which is unique up to positive linear transformations.\footnote{Also see \citet{Bradley1998}, for an alternative way to secure uniqueness.}

We have introduced the most prominent representation theorems for expected utility theory.\footnote{A helpful, more technical and more detailed overview of representation theorems can be found in \citet{Fishburn1981}.} What do these representation theorems show? Each of them shows that if an agent's preferences abide by certain axioms, and certain structural conditions are met, her preferences can be represented by a utility (and probability) function (or families thereof) such that she prefers an act to another just in case its expected utility is higher. Agents who abide by the axioms can thus be represented as expected utility maximisers.

What these kinds of results show depends to some extent on the purpose we want to put our theory to. But it also depends on how we interpret the utilities and probabilities expected utility theory deals with. Section \ref{s3} gives an overview of these interpretations and then returns to the question of what the representation theorems can show.

\section{Interpretations of Expected Utility Theory} \label{s3}

\subsection{Interpretations of utility}\label{subs31}

Some of the earliest discussions of choice under uncertainty took place in the context of gambling. The idea that gamblers maximise some expected value first came up in correspondence between \citet{FermatPascal1654}. Pascal, who formulated the expected value function in this context, thought of the value whose expectation should be maximised as money. This is natural enough in the context of gambling. Similarly, in this context it is natural to think of the probabilities involved as objective, and fixed by the parameters of the game.

However, money was soon replaced by the notion of utility as the value whose expectation is to be maximised. This happened for two interrelated reasons. First, the same amount of money may be worth more or less to us depending on our circumstances. In particular, we seem to get less satisfaction from some fixed amount of money the more money we already have. Secondly, the norm to maximise expected monetary value has some counterintuitive consequences. In particular, we can imagine gambles that have infinite monetary value, that we would nevertheless only pay a finite price for. Nicolas Bernouilli first demonstrated this with his famous St. Petersburg Paradox.\footnote{Bernouilli proposed a gamble in which a coin is thrown repeatedly. If it lands heads the first time, the player gets \$2. If it lands tails, the prize is doubled, and the coin thrown again. This procedure is repeated indefinitely. The expected value of the resulting gamble is thus $\$2 \cdot \frac{1}{2} + \$4 \cdot \frac{1}{4} + \$8 \cdot \frac{1}{8} +...$, which is infinite. However, most people would only pay a (low) finite amount for it.}

In response to these problems, Daniel \citet{Bernouilli1738} and Gabriel Cramer independently proposed a norm to maximise expected utility rather than expected monetary value. However, this raises the problem of how to interpret the notion of utility. One strand of interpretations takes utility to be a real psychological quantity that we could measure. Let us call such interpretations of utility `realist'. Early utilitarians adopted a realist interpretation of utility. For instance, \citet{Bentham1789} and \citet{Mill1861} thought of it as pleasure and the absence of pain.

Note, however, that these utilitarians were interested in defining utility for the purpose of an ethical theory rather than a theory of rationality. One problem with interpreting utility as pleasure in the context of expected utility theory is that the theory then seems to imply that true altruism can never be rational. If rationality requires me to maximise my own expected pleasure, then I can never rationally act so as to increase somebody else's happiness at my own expense.

For this and other reasons modern realists typically think of utility as a measure of the strength of an agent's desire or preference, or her level of satisfaction of these desires or preferences. I may strongly desire somebody else's happiness, or be satisfied if they achieve it, even if that does not directly make me happy.\footnote{This is also the interpretation adopted by several later utilitarians, such as \citet{Hare1981} and \citet{Singer1993}.} \citet{Jeffrey1983}, for instance, speaks of desirabilities instead of utilities, and interprets them as degrees of desire (p. 63). The corresponding realist interpretation of the probabilities in expected utility theories is usually that of subjective degrees of belief.

The representation theorems described in Section \ref{s2} have, however, made a different kind of interpretation of utility (and probability) possible, and popular. These representation theorems show that preferences, if they conform to certain axioms, can be represented with a probability and utility function, or families thereof. And so, encouraged by these results, many decision theorists think of utility and probability functions as mere theoretical constructs that provide a convenient way to represent binary preferences. For instance, \citet{Savage1954} presents his theory in this way.  Importantly, on this interpretation, we cannot even speak of probabilities and utilities in the case where an agent's preferences do not conform with the axioms of expected utility theory. Let us call these interpretations of utility and probability `constructivist'.\footnote{See \citet{Dreier1996} and \citet{Velleman1993} for defenses of constructivism. \citet{Buchak2013} draws slightly different distinctions. For her, any view on which utility is at least partially defined with respect to preferences counts as constructivist. Since this is compatible with holding that utility is a psychologically real quantity, she allows for constructivist realist positions. The position that utility expresses strength of desire, for her, is such a position. I will count this position as realist, and not constructivist.}

\subsection{The significance of the representation theorems}\label{subs32}

Whether we adopt a realist or a constructivist interpretation of utility matters for how expected utility theory can serve the three purposes of decision theory described in Section \ref{subs13}, and for what the representation theorems presented in Section \ref{s2} really establish. Let us first look at the interpretive project. As already mentioned, those interested in the interpretive project have mostly been interested in inferring an agent's beliefs and desires from her choice behaviour. If that is the goal, then the probabilities and utilities involved in decision theory should at least be closely related to desires and beliefs. Under the assumption that agents maximise their utility and probability functions, thus understood, we can hypothesise, perhaps even derive, probability and utility functions that motivate an agent's actions.

How could the representation theorems we described in Section \ref{s2} help with this project? They go some way towards showing that beliefs and desires can be inferred from an agent's choice behaviour. But the following assumptions are also needed for this project to succeed:
\begin{enumerate}
\item The agent's choice behaviour must reflect her preferences, at least most of the time. This assumption is more likely to be met if we think of preferences as a dispositions to choose, rather than as judgements of choiceworthiness.
\item The axioms of the representation theorems must be followed by the agent, at least most of the time. If we want to use expected utility theory to deduce an agent's beliefs and desires, then the agent's preferences have to be representable by an expected utility function. While we can interpret the axioms as rationality constraints, these cannot be the kinds of constraints that people fail to meet most of the time. In particular, if we want to employ expected utility theory for Davidson's `radical interpretation', then the choice behaviour of agents who fail to abide by the axioms will turn out to be unintelligible.
\item The probabilities and utilities furnished by the representation theorem must correspond to the agent's actual beliefs and desires.
\end{enumerate}

Assumption 2 is controversial for the reasons described in Sections \ref{s4} and \ref{s5}. But assumption 3 is also problematic. The representation theorems only show that an agent who abides by the axioms of the various representation theorems can be represented as an expected utility maximiser. But this is compatible with the claim that the agent can be represented in some other way. It is not clear why the expected utility representation should be the one which furnishes the agent's beliefs and desires.\footnote{This question was raised, for instance, by \citet{Zynda2000}, \citet{Hajek2008} and \citet{MeachamWeisberg2011}. \citet{Zynda2000} argues that the representation theorems alone cannot show that agents do or should have probabilistic degrees of belief. \citet{MeachamWeisberg2011} provide a number of arguments why the representation theorems alone cannot serve as the basis of decision theory.}

To answer this challenge, the best strategy seems to be to provide further arguments in favour of expected utility maximisation, and in favour of probabilistic beliefs, apart from the plausibility of the axioms of the representation theorems. Suppose we think it is plausible that agents should have probabilistic degrees of belief, and should maximise the expected degree of satisfaction of their desires. And suppose we also think that our preferences are closely related to our desires. Then if, given some plausible axioms, these preferences can be given an expected utility representation, we seem to have good reason to think that the utilities and probabilities furnished by the representation theorem correspond to our degrees of belief and strength of desire.

Setting aside the question of why we might want to have probabilistic degrees of belief, what could such realist arguments for expected utility maximisation be? Note that, for the purposes of the interpretive project, these arguments have to not only be normatively compelling, but also convince us that ordinary agents would be expected utility maximisers. One type of argument appeals to the advantages of being an expected utility maximiser when making decisions in a dynamic context. These will be covered in Section \ref{s7}. \citet{Pettigrew2014} makes another argument: for most realists, utility is supposed to capture everything an agent cares about. If that is true, then it seems plausible to say that in uncertain situations, I should be guided by my best estimate of how much utility I will get. We can appeal to results in \citet{deFinetti1974} to argue that an agent's best estimate of a quantity is her subjective expectation. This is so because any estimate of the quantity that is a weighted sum different from the expectation will be accuracy dominated by an expectational estimate: the expectational estimate will be closer to the true value no matter what happens. Thus, I should maximise my expected utility.

So far, we have assumed a realist interpretation of utility and probability. Note, however, that expected utility theory could still be explanatorily useful even if a constructivist interpretation of utility and probability are adopted. It is often argued that the representation theorems show that the utility and probability functions allow for a simpler and more unified representation of an agent's preferences: all the agent's preferences can be described with one utility and probability function. This could be seen to make them more intelligible. In fact, \citet{Velleman1993} argues that being an expected utility maximiser makes an agent more intelligible to herself and others, and that this gives her a reason to be an expected utility maximiser.

Let us now turn to the action-guiding and normative projects. These projects will lead to quite different prescriptions depending on whether utility is interpreted in a realist or in a constructivist sense. Suppose that we are constructivists about utility. In that case, there is a sense in which the prescription to maximise expected utility does not make any sense. If one abides by the axioms of one's favourite representation theorem, one's preferences are representable as expected utility maximising. To maximise expected utility, there is nothing more one needs to do, apart from act according to the preferences over acts one already has.  But if one's preferences do not abide by the axioms, on the other hand, one simply does not have a utility function whose expectation one could maximise.

Consequently, constructivists often interpret the prescription of expected utility theory as a prescription to have preferences such that one can be represented as an expected utility maximiser. That is, one should abide by the axioms of expected utility theory. For the action-guiding project, this means that, as an agent, I should have preferences such that they abide by the axioms of expected utility theory. For the normative project, it means that we judge an agent to be irrational if she has preferences that violate the axioms. This is why constructivists often interpret expected utility theory as a theory about what it means to have coherent preferences or ends, rather than as a theory of means-ends rationality.

For realists, however, the prescription to maximise expected utility makes sense even independently of the representation theorems canvassed in Section \ref{s2}. Consider first the action-guiding project, which aims to interpret expected utility theory as a theory that can guide an agent in deciding what to do. If utility is just my strength of desire, and probability is my degree of belief, and I have introspective access to these, then I can determine the expected utility of the various acts open to me. I can do so without considering the structure of my preferences, and whether they abide by the axioms of expected utility theory. Expected utility theory is then action-guiding without appeal to representation theorems. But note that the advice to maximise expected utility is only useful to agents if they really have such intuitive access to their own degrees of belief and strength of desire.\footnote{Also see \citet{Bermudez2009} on this claim.}

Similarly, if we are realists and our interests are normative, we can judge an agent to be irrational by considering her utilities and degrees of belief, and determining whether she failed to maximise expected utility. This is because there will be facts about the agent's utilities and probabilities even if she fails to maximise expected utility. Realists about utility and probability can also help themselves to the realist arguments for expected utility maximisation just mentioned. For them, the normative force of expected utility theory does not depend solely on the plausibility of the axioms of expected utility theory. If we adopt a realist interpretation of utility and probability, it is also easier to argue that expected utility theory provides us with a theory of instrumental rationality. Maximising expected utility could be seen as taking the means towards the end of achieving maximum utility. However, realists will also have to provide an argument that this is a goal rational agents ought to have.

\subsection{Causal and evidential interpretations of expected utility theory}\label{subs33}

We have said that the probabilities involved in expected utility theory are usually interpreted as subjective degrees of belief, at least by realists. As we have seen, Jeffrey, Joyce, and others have advocated a conditional expected utility theory. In conditional expected utility theory, agents determine an act's expected utility by weighting utilities by the different states' probabilities conditional on the act in question being performed. Above, we called this probability $p_{A} (S)$. How this probability is to be interpreted is a further important interpretive question. The main disagreement is about whether it should be given a causal or an evidential interpretation. Jeffrey himself had worked with an evidential interpretation, while causal decision theorists, such as \citet{GibbardHarper1978}, \citet{Armendt1986}, or \citet{Joyce1999}\footnote{Joyce also first showed that the two interpretations can be given a unified treatment in a more general conditional expected utility theory.} have given it a causal interpretation.

The difference between these two interpretations is brought out by the famous Newcomb Problem, first introduced by \citet{Nozick1969}. In this problem, we imagine a being who is very reliable at predicting your decisions, and who has already predicted your choice in the following choice scenario. You are being offered two boxes. One is opaque and either has no money in it, or \$1,000,000. The other box is clear, and you can see that it contains \$1,000. You can choose to either take only the opaque box, or to take both boxes. Under normal circumstances, it would seem clear that you should take both boxes. Taking the clear box gives you \$1,000 more no matter what.

The complication, however, is that the being's prediction about your action determines whether there is money in the opaque box or not. If the being predicted that you will take two boxes, then there is no money in the opaque box. If the prediction was that you will take only the opaque box, there will be money in it. Since the being's prediction is reliable, those who take only one box tend to end up with more money than those who take two boxes.

Note that while this case is unrealistic, there are arguably real-life cases that resemble the Newcomb Problem in its crucial features. In these cases, the acts available to an agent are correlated with good or bad outcomes even though these are not causally promoted by the act. This happens in medical cases, for instance, if a behavioural symptom is correlated with a disease due to a common cause. Before the causal link between smoking and lung cancer was firmly established, interested parties hypothesised that there may be a common cause which causes both lung cancer, and the disposition to smoke. If that were right, smoking would not cause lung cancer, but merely give you evidence that you are more likely to develop it.\footnote{See \citet{Price1991} for more examples.}

Evidential and causal decision theory come apart in their treatment of these cases. Evidential decision theory traditionally interprets $p_{A} (S)$ as a standard conditional probability:
$$p_{A} (S) = \frac{p(A\&S)}{p(A)}.$$
According to this interpretation, the probability of the state where there is \$1,000,000 in the opaque box conditional on taking only one box is much higher than the probability of the state where there is \$1,000,000 in the opaque box conditional on taking two boxes. This is because the act of taking only one box provides us with evidence that the prediction was that you would take only one box, in which case there is money in the opaque box. And so expected utility maximisation would tell you to take only one box.

Causal decision theorists take issue with this, because at the time of decision, the agent's actions have no more influence on whether there is money in the opaque box or not. Either there is or there isn't already money in the box. In either case, it is better for you to take two boxes, as Table \ref{t4} illustrates. This kind of dominance reasoning speaks in favour of taking both boxes.

\begin{table}
\centering
\label{my-label}
\begin{tabular}{lllll}
\cline{2-3}
\multicolumn{1}{l|}{}                & \multicolumn{1}{l|}{Prediction: one box} & \multicolumn{1}{l|}{Prediction: two boxes} &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Take one box}   & \multicolumn{1}{l|}{\$1,000,000}         & \multicolumn{1}{l|}{\$0}                   &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Take two boxes} & \multicolumn{1}{l|}{\$1,001,000}         & \multicolumn{1}{l|}{\$1,000}               &  &  \\ \cline{1-3}
                                     &                                          &                                            &  &
\end{tabular}
\caption{The Newcomb Problem}
\label{t4}
\end{table}

Causal decision theory allows for this by giving $p_{A} (S)$ a causal interpretation. It measures the causal contribution of act $A$ to whether state $S$ obtains. Following a proposal by \citet{Stalnaker1972}, \citet{GibbardHarper1978} use the probability of a conditional in their causal decision theory, instead of a conditional probability. In particular, they use the probability of the conditional that an outcome would occur if an action was performed.\footnote{\citet{Lewis1981} shows that if the right partition of acts, states and outcomes is used, Savage's decision theory will give the same recommendations as Gibbard and Harper's, and is thus a type of causal decision theory.}

In the Newcomb Problem, neither the act of taking nor the act of not taking the clear box make any causal contribution to whether there is money in the opaque box. And so, on the causal interpretation, $p_{A} (S)$  just equals the unconditional probability $p (S)$ in both cases. And then dominance reasoning becomes relevant.

Note, however, that it is controversial whether taking both boxes really is the rational course of action in the Newcomb Problem. Those who advocate `one-boxing', such as \citet{Horgan1981} and \citet{Horwich1987}, point out that one-boxers end up faring better than two-boxers. It is also controversial whether evidential decision theory really does yield the recommendation to one-box if the problem is represented in the right way: \citet{Eells1981} argues that evidential decision theory, too, recommends two-boxing.

\citet{Jeffrey1983} himself supplements evidential decision theory with a ratifiability condition, which allows him to advocate two-boxing. The condition claims that an agent should maximise expected utility relative to the probability function she will have once she finally decides to perform the action. In the Newcomb Problem, only two-boxing is ratifiable. If the agent decided to one-box, she would then be fairly certain that there is money in the opaque box, and then she will wish she had also taken the second box. If she decides to two-box, she will be fairly certain that there is no money in the opaque box, and she will be glad that she at least got the \$1,000.\footnote{The status of the ratifiability condition is still a part of the contemporary debate on causal decision theory. One open question is what decision should be favoured in cases of decision instability, where no action is ratifiable, like in Gibbard and Harper's Death in Damascus case (see \citet{GibbardHarper1978}, and \citet{Egan2007} for further, similar cases). \citet{Arntzenius2008} and \citet{Joyce2012} argue for ways of dealing with this problem. The ratifiability condition also helps to illuminate certain equilibrium concepts in game theory (see \citet{JoyceGibbard1998}).}

\section{Incompleteness and Imprecision} \label{s4}

Several important challenges to expected utility theory have to do with the fact that expected utility theory asks us to have attitudes that are more extensive and precise than the preferences ordinary decision makers have. In fact, in many cases it does not seem irrational to have attitudes that are in some way imprecise or incomplete. And so the problems discussed in the following arise both for the interpretive as well as for the action-guiding and normative uses of decision theory.

The challenge takes different forms for constructivists and realists. For constructivists, imprecision and incompleteness will manifest as violations of the axioms of the representation theorems presented in Section \ref{s2}. As we have seen, all of these representation theorems assume that the agent's preference relation forms a weak ordering of the elements in its domain. This means that the preference relation must be transitive and complete. Both assumptions are controversial for related reasons. Completeness is controversial because it asks agents to have a more extensive set of preferences than they actually have. Transitivity is controversial in cases where an agent's desires are coarse-grained, as will be explained below. For realists, a related challenge is that both our degrees of belief and our strength of desire are not precise enough to allow for representation in terms of a precise probability and utility function.

\subsection{Incompleteness}\label{subs41}

To start with the completeness condition, the worry here is that agents simply do not have preferences over all the elements of the set the decision theory asks them to have preferences over. For instance, if I have lived in Germany all my life, I might simply have no preference between living in Nebraska and living in in Wyoming. It's not that I have never heard of these places. The question would just never occur to me. It might then neither be the case that I prefer Nebraska to Wyoming nor that I prefer Wyoming to Nebraska. I am also not indifferent between the two. I might simply have no preference. But if these outcomes are part of the set of outcomes the decision theory asks me to have preferences over, then this means that I am violating the completeness condition.

Similar claims are often made about cases of incommensurable values. In a famous example due to \citet{Sartre1945}, a young man has to choose between caring for his sick mother and joining the French Resistance. The two options here are often said to involve incommensurable values: on the one hand, responsibility to one's family, and on the other hand, fighting for a just cause. In these kinds of cases, too, we might want to say that the young man is neither indifferent, nor does he prefer one option to the other. And here, this is not because the question of what he prefers has never occurred to the man. He may in fact think long and hard about the choice. Rather, he has no preference because the values involved are incommensurable.

These kinds of examples are more convincing if our notion of preference is that of a judgement of choiceworthiness. In these examples, agents have not made, or are unable to make judgements of choiceworthiness about some of the elements of the relevant set. If one thinks of preference as disposition to choose instead, one might think that even if an agent never thought about a particular comparison of outcomes, there can still be a fact of the matter what she would be disposed to choose if she faced the choice. Moreover, if this is our notion of preference, we simply draw no distinction between indifference and incommensurability.\footnote{In fact, \citet{Joyce1999} considers this an important argument against more behaviourist interpretations of preference.}

However, this alternative notion of preference may get into trouble when some of the acts in the relevant set are ones that the agent could not possibly choose between. The completeness condition in standard expected utility theory may require the agent to have what \citet{Broome1991} calls `impractical preferences'. For instance, it might require an agent to have a preference between
\begin{description}
\item[$O_1:$] an orange, and
\item[$O_2:$] an apple when the alternative is a banana
\end{description}
Choosing between these alternatives is impossible in the sense that $O_2$ will not come about unless the alternative is a banana, not an orange. And so it seems like we cannot determine the agent's choice disposition between them.

Incompleteness in preference is often dealt with by replacing the completeness axiom in the various representation theorems with a condition of {\em coherent extendibility}.\footnote{This is the strategy taken by \citet{Kaplan1983}, \citet{Jeffrey1983}, and \citet{Joyce1999}.} That is, we only require that an agent's preferences are such that we could extend her set of preferences in a way that is consistent with the other axioms of the representation theorem. The problem with this strategy is that any representation in terms of probability or utility that the representation theorem furnishes us with will only be a representation relative to an extension. There will usually be several extensions that are consistent with an agent's incomplete preferences and the axioms of the theorem. And thus, there will be several possible representations of the agent's preferences. The representation theorem will no longer furnish us with a unique probability function, and a utility function that is unique up to positive linear transformations. For this reason, incompleteness of preference is often associated with imprecise probabilities.

\subsection{Imprecise probabilities}\label{subs42}

There is an active field of research investigating imprecise probabilities.\footnote{See \citet{Bradley2015} and \citet{Mahtani2019} for helpful overviews of the literature. For an introduction to the theory of imprecise probabilities, see \citet{Augustin2014}.} These imprecise probabilities are usually represented by families of probability functions. And families of probability functions is exactly what the representation theorems furnish us with if the completeness condition is replaced by a coherent extendibility condition. While this gives even a constructivist reason to engage with imprecise probabilities, there are also various realist arguments for doing so. Many formal epistemologists agree that sharp degrees of belief that can be expressed with a sharp probability function are both psychologically unrealistic, and cannot be justified in situations where there is insufficient evidence.\footnote{For examples of these claims, see, for instance, \citet{Levi1980} and \citet{Kaplan1996}. When an agent cannot assign a sharp probability to states, we sometimes speak of decision-making under indeterminacy or ignorance, as opposed to merely uncertainty.} If we believe that the probabilities in decision theory should accurately describe our belief states, the probabilities in decision theory should then be imprecise.

Another motivation for engaging with imprecise probabilities is that this allows us to treat states or outcomes to which the agent can assign precise probabilities differently from states or outcomes to which the agent cannot assign precise probabilities. This may allow us to make sense of the phenomenon of {\em ambiguity aversion}. Ambiguity aversion occurs in situations where the probabilities of some states are known, but the agent has no basis for assigning probabilities to some other states. In such situations, many agents are biased in favour of lotteries where the probabilities are known. For instance, take the following example from \citet{CamererWeber1992}:\footnote{\citet{CamererWeber1992} also provide an overview of the empirical evidence of this phenomenon.}

\begin{displayquote}
Suppose you must choose between bets on two coins. After flipping the first coin thousands of times you conclude it is fair. You throw the second coin twice; the result is one head and one tail. Many people believe both coins are probably fair ($p(\text{head}) = p(\text{tail})=.5$) but prefer to bet on the first coin, because they are more confident or certain that the first coin is fair. (p. 326)
\end{displayquote}

Standard expected utility theory cannot make sense of this, since it does not allow us to distinguish between different degrees of uncertainty. In standard expected utility theory, every state is assigned a precise probability. As a result, ambiguity aversion can lead an agent to violate the axioms of the different representation theorems. In particular, ambiguity aversion can result in violations of separability (see Section \ref{s5}) as in the famous Ellsberg Paradox.\footnote{See \citet{Ellsberg1961}.The Ellsberg Paradox runs as follows: you are given an urn that you know contains 90 balls. 30 of them are red. The remaining 60 are either black or yellow, but you don't know what the distribution is. Now first, you are offered the choice between receiving \$100 if a red ball is drawn, and receiving \$100 if a black ball is drawn. Most people choose the former. Then, you are offered the choice between receiving \$100 if a red or yellow ball is drawn, and receiving \$100 if a black or yellow ball is drawn. Here, most people choose the latter. These preferences display ambiguity aversion. They are not consistent with a stable assignment of precise subjective probabilities to the drawing of a yellow or black ball, combined with the assumption of expected utility maximisation.} Nevertheless, ambiguity aversion is common and does not seem irrational. Imprecise probabilities may help us to better model ambiguity, and thus hold the promise to help us rationalise ambiguity averse preferences.

There are epistemological objections to using sets of probabilities to represent beliefs.\footnote{See, for instance, the problem of {\em dilation}. Dilation occurs when an agent's beliefs become less precise when she updates on a piece of evidence. The phenomenon was first introduced by \citet{SeidenfeldWasserman1993} and is argued to be problematic for imprecise probability theory in \citet{White2010}. See \citet{Joyce2011}, \citet{BradleySteele2014b} and \citet{PedersenWheeler2014} for critical discussion.} But another common objection to using imprecise probabilities is that they lead to bad decision-making.\footnote{See, for instance, \citet{Williamson2010}.} How could decision-making with imprecise probabilities proceed? We can use each probability function in the family in order to calculate an expected utility for each act open to the agent. But then each act will be associated with a family of expected utilities, one for each member of the family of probability functions. And so the agent cannot simply maximise expected utility anymore. The question then becomes how we should make decisions with these sets of probabilities and expected utilities.

One type of simple proposal that appears in the literature is the following principle, sometimes called {\em Liberal}: an act which maximises expected utility for every probability function in the family is obligatory. And any act which maximises expected utility for some probability function in the family is permitted.\footnote{See \citet{White2010}, \citet{Williams2014}, \citet{Moss2015}.} For an overview of other choice rules, see \citet{Troffaes2007}.

\citet{Elga2010} raises an important challenge for all such choice rules. If they are permissive, as {\em Liberal} is, then they will allow us to make choices in a series of bets that leave us definitely worse off. But if they are not permissive, and always recommend a single action, they undercut one main motivation for using imprecise probabilities in the first place. In that case, they will pin down precise betting odds for an agent. But, Elga argues, if we think that the evidence does not license us to use a precise probability, it would be strange if it determined precise betting odds. Moreover, these betting odds, if they abide by the axioms of expected utility theory, could be used to infer a precise probability using the representation theorems discussed above.\footnote{However, note that there are choice rules that determine precise betting odds that do not reduce to expected utility maximisation, such as the one introduced by \citet{SahlinWeirich2014}.}

Elga's argument bears resemblance to other dynamic arguments against violations of standard expected utility theory, which will be discussed in Section \ref{s7}. It may be challenged on similar grounds. There may be dynamic choice strategies available to agents that guard them against making sure losses in dynamic choice problems. In fact, \citet{Williams2014} claims that agents using his choice rule can make their choices `dynamically permissible' by only considering some of the probability functions in the family to be `live' at any one point. \citet{BradleySteele2014}, too, argue that agents with imprecise credences can make reasonable choices in dynamic settings.

\subsection{Imprecise utility and intransitivity}\label{subs43}

One might expect there to be a literature on imprecision with regard to utilities similar to the one on imprecise probabilities. For one, replacing the completeness condition with a condition of coherent extendibility will not only lead to a family of probability representations, it will also result in a corresponding family of utility representations. Moreover, there might be similar realist arguments that could be made in favour of imprecise strength of desire or degree of preference. Some of the examples of incompleteness, such as the cases involving incommensurable values, could be described as examples where it is unclear to what degree an agent desires the goods in question, or how they compare. Such cases are also often described as cases of `vague preference'. However, imprecise utilities and vague preferences are so far mostly discussed in the mathematical and economic literature. \citet{Fishburn1998} suggests a probabilistic approach to studying vague preferences, while most of the literature uses fuzzy set theory. \citet{Salles1998} provides an introduction to that approach.

There is a certain kind of lack of precision in our attitudes that does not result in vague preferences or incompleteness of preference. Instead, this lack of precision leads to a failure of transitivity, and is thus nevertheless problematic for expected utility theory. Intransitivity arises for outcomes that the agent finds indistinguishable with regard to some of the things she values. The problem is brought out most clearly by the Self-Torturer Problem, introduced by \citet{Quinn1990}. It runs as follows: a person has an electric device attached to her body that emits electric current which causes her pain. The device has a large number of settings, such that the person is unable to tell the difference in pain between any two adjacent settings. However, she can tell the difference between settings that are sufficiently far apart. In fact, at the highest settings, the person is in excruciating pain, while at the lowest setting, she is painless. Each week, the person can turn the dial of the device up by one setting, in exchange for \$10,000.

Let us call the settings of the dial $D_0, D_1, D_2,..., D_{1000}$. In this problem, the following set of intransitive preferences seems to be reasonable for a person who prefers less pain to more pain, and more money to less:
$$D_0 \prec D_1 \prec D_2 \prec ... \prec D_{1000} \prec D_0.$$
At the highest settings, the person is in such excruciating pain that she would prefer being at the lowest setting again to having her fortune. At the same time, if turning the dial up by one setting results in a level of pain that is indistinguishable from the previous, it seems that taking the \$10,000 is always worth it, no matter how much pain the agent is already in.

An agent who has the self-torturer's preferences is clearly in trouble. In the original example, she can never turn the dial down again once she has turned it up. If she always follows her pairwise preferences, she will end up at the highest setting. This is obviously bad for her, by her own lights: there are many settings she would prefer to the one she ends up at. If, on the other hand, we suppose that the agent can go back to the first setting in the end, the problem is that she could be `money-pumped'.\footnote{Money pumps were first introduced as an argument for transitivity by \citet{DavidsonMcKinseySuppes1955}.} If the agent has a strict preference for the lowest setting over the highest setting, she should be willing to pay some positive amount of money on top of giving up all her gained wealth for going back to the first setting. She will end up having paid money for ending up where she started.

Advocates of standard expected utility theory may point out that these observations just show why it is bad to have intransitive preferences. However, critics, such as \citet{Andreou2006} and \citet{TenenbaumRaffman2012}, point out that while these are problematic consequences of having the self-torturer's preferences, there seems to be nothing wrong with the self-torturer's preferences per se. If the agent's relevant underlying desires are those for money and the absence of pain, but the agent cannot distinguish between the levels of pain of two adjacent settings, then there is nothing in the agent's desires concerning the individual outcomes that could speak against going up by one setting. If we think that preferences should accurately reflect our underlying desires concerning the outcomes, the self-torturer's preferences seem reasonable.

Indeed, proponents of expected utility theory acknowledge that it is somewhat unsatisfactory to simply declare the self-torturer's preferences irrational. They have hence felt pressed to give an explanation of why the self-torturer's preferences are unreasonable, despite appearances. \citet{ArntzeniusMcCarthy1997}, and \citet{VoorhoeveBinmore2006} have made different arguments to show that rational agents would hold that there is an expected difference in pain between two adjacent settings at least somewhere in the chain.

Critics note that it is only in the context of the series of choices she is being offered that the self-torturer's preferences become problematic. And so instead of declaring the self-torturer's preferences irrational, we may instead want to say that in some cases, it is rational for the agent to act against her punctate preferences. \citet{Andreou2006} argues that the intransitive preferences of the self-torturer ought to be revised to be transitive for the purpose of choice only. \citet{TenenbaumRaffman2012} note that the underlying problem in the self-torturer's case is that the agent's end of avoiding pain is {\em vague}. It is not precise enough to distinguish between all the different outcomes the decision theory may ask her to evaluate, and that she in fact may have to choose between. They claim that vague goals that are realised over time may ground permissions for agents to act against their punctate preferences. And so this is another type of imprecision in our attitudes which may call for a revision of standard expected utility theory.

\section{Separability}\label{s5}

\subsection{The separability assumption}\label{subs51}

The imprecision and incompleteness of our attitudes discussed in Section \ref{s4} may be a problem for expected utility theory even in the context of certainty. But another important type of criticism of expected utility theory has to do with the assumptions it makes about choice under uncertainty specifically. All the representation theorems canvassed in Section \ref{s2} make use of a similar kind of axiom about choice under uncertainty. These axioms are versions of what \citet{Broome1991} calls {\em separability}. The idea here is that what an agent expects to happen in one state of the world should not affect how much she values what happens in another, incompatible state of the world. There is a kind of independence in value of outcomes that occur in incompatible states of the world. Separability is largely responsible for the possibility of an expected utility representation. Separability is a controversial assumption, for the reasons explained in Sections \ref{subs52} and \ref{subs53}. Here, I present the versions of the separability assumption used in the representation theorems introduced in Section \ref{s2}.

In von Neumann and Morgenstern's representation theorem (see Section \ref{subs22}), separability is expressed by the independence axiom. Let $\mathscr{L}$ be the space of lotteries over all possible outcomes. Then independence requires the following:
\begin{description}
\item[Independence:] For all $L_x, L_y, L_z \in \mathscr{L}$ and all $p \in (0, 1)$, $L_x \succcurlyeq L_y$ if and only if $p \cdot L_x + (1 - p) \cdot L_z \succcurlyeq p \cdot L_y + (1 - p) \cdot L_z$.
\end{description}
Independence claims that my preference between two lotteries will not be changed when those lotteries become sub-lotteries in a lottery which mixes each with some probability of a third lottery. For instance, suppose I know I get to play a game tonight. I prefer to play a game that gives me a 10\% chance of winning a pitcher of beer to a game that gives me a 20\% chance of winning a pint of beer. The independence axiom says that this preference will not be affected when the chances of me getting to play at all today change. The possibility of not playing at all tonight should not affect how I evaluate my options in the case that I do get to play.

In Savage's framework (see Section \ref{subs23}), separability is expressed by his famous sure-thing principle. To state it, we need to define a set of events, which are disjunctions of states. Let $A_i (E)$ be the act $A_i$ when event $E$ occurs. The sure-thing principle then requires the following:
\begin{description}
\item[Sure-thing principle:] For any two actions $A_i$ and $A_j$, and any mutually exclusive and exhaustive events $E$ and $F$, if $A_i (E) \succcurlyeq A_j (E)$ and $A_i (F) \succcurlyeq A_j (F)$, then $A_i \succcurlyeq A_j$
\end{description}
The idea behind the sure-thing principle is that an agent can determine her overall preferences between acts through event-wise comparisons. She can partition the set of states into events, and compare the outcomes of each of her acts for each event separately. If an act is preferred given each of the events, it will be preferred overall. That is, if a particular act is preferred no matter which event occurs, then it is also preferred when the agent does not know which event occurs.

In Jeffrey's decision theory (see Section \ref{subs24}), separability is expressed by the averaging axiom. Remember that for him, acts, states and outcomes are all propositions, and all objects of preference. The averaging axiom claims the following:
\begin{description}
\item[Averaging:] If $A$ and $B$ are mutually incompatible propositions, and $A \succcurlyeq B$, then $A \succcurlyeq (A$ or $B) \succcurlyeq B$.
\end{description}
The averaging axiom claims that how much an agent values a disjunction should depend on the value she assigns to the disjuncts in such a way that the disjunction cannot be more or less desirable than any of the disjuncts. When the propositions involved are outcomes that occur in different states of the world, this requirement, too, expresses the idea that there is an independence in value between what happens in separate states of the world. Knowing only that I will end up with one of two outcomes cannot be worse than ending up with any of the individual outcomes.

Assuming separability for preferences in the way that the independence axiom, the sure-thing principle and the averaging axiom do ensures that the utility representation has an important separability feature as well. As we have seen, in expected utility theory, the overall value of an action can be represented as a probability-weighted sum of the utilities of the outcomes occurring in separate states. This means that the value contribution of an outcome in one state will be independent of the value contribution of an outcome of another state, holding the probabilities fixed. And so the separability of the value of outcomes in separate states is captured by equating the value of an action with its expected utility. If separability is problematic, it is thus problematic independently of any representation theorem. In particular, this means that it is also problematic for realists.

\subsection{Violations of separability}\label{subs52}

To see how separability may fail, consider the following decision problem, known as the Machina Paradox.\footnote{See, for instance, \citet{Mas-Colell1995}, chapter 6.} Suppose you prefer actually going to Venice to staying at home and watching a movie about Venice. You also prefer watching a movie about Venice to doing nothing and being bored. You are now offered the lotteries described in Table \ref{t5}. Suppose that each lottery ticket is equally likely to be drawn, so that, if we want to apply von Neumann and Morgenstern's framework, each lottery ticket has a probability of 1\%.

\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\cline{2-3}
\multicolumn{1}{l|}{}           & \multicolumn{1}{l|}{Tickets 1--99} & \multicolumn{1}{l|}{Ticket 100}         &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Lottery A} & \multicolumn{1}{l|}{Go to Venice}   & \multicolumn{1}{l|}{Bored at home}      &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Lottery B} & \multicolumn{1}{l|}{Go to Venice}   & \multicolumn{1}{l|}{Movie about Venice} &  &  \\ \cline{1-3}
                                &                                     &                                         &  &
\end{tabular}
\caption{Machina's Paradox}
\label{t5}
\end{table}

Many people would prefer lottery A to lottery B in this context. Clearly, if I am so unlucky as to draw ticket 100, I'd rather not have to watch a movie reminding me of my misfortune. However, my preferences, as stated, violate the independence axiom and sure-thing principle. It is also clear why this violation of separability occurs. What happens in alternative, incompatible states of the world, that is, what might have been, clearly matters for how I evaluate the outcome of watching a movie about Venice. If there was a big probability that I could have gone to Venice, I will evaluate that outcome differently from when there was no such possibility. In this case, the reason for an interdependence in value between outcomes in alternative states of the world is disappointment: the movie about Venice heightens my disappointment by reminding me of what I could have had.

The natural response to this kind of problem is to say that the outcomes in the decision problem as I stated it were under-described. Clearly, the feeling of disappointment is a relevant part of the outcomes of lottery B. There is nothing irrational about wanting to avoid disappointment, and many agents do. Thus, according to all the rules for the individuation of outcomes discussed in Section \ref{subs14}, watching a movie about Venice with disappointment should be a different outcome from watching a movie about Venice without disappointment. And then, no violation of separability occurs.

This seems to be a valid response in the case of Machina's Paradox. However, there are other violations of separability that arguably cannot be given the same treatment. One famous case that seems to be more problematic is the Allais Paradox, introduced in \citet{Allais1953}. It runs as follows. First a subject is offered a choice between \$1 million for certain on the one hand, and an 89\% chance of winning \$1 million, a 10\% chance of winning \$5 million, and a 1\% chance of winning nothing on the other. What she will get is decided by a random draw from 100 lottery tickets. Many people choose \$1 million for certain when offered this choice. Next, the subject is offered the choice of either a 10\% chance of \$5 million, and nothing otherwise on the one hand, or an 11\% chance of \$1 million, and nothing otherwise on the other. Again, this is decided by the draw of a lottery ticket. Here, most people pick the first lottery, that is, the lottery with the higher potential winnings.

While this combination of preferences seems sensible, it in fact violates independence and the sure-thing principle, given a natural specification of the outcomes involved. This becomes evident when we represent the two choices in decision matrices, as in Tables \ref{t6} and \ref{t7}.

\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\cline{2-4}
\multicolumn{1}{l|}{}           & \multicolumn{1}{l|}{Tickets 1--89} & \multicolumn{1}{l|}{Tickets 90--99} & \multicolumn{1}{l|}{Ticket 100}  &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Lottery C} & \multicolumn{1}{l|}{\$1 million}    & \multicolumn{1}{l|}{\$5 million}     & \multicolumn{1}{l|}{\$0}         &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Lottery D} & \multicolumn{1}{l|}{\$1 million}    & \multicolumn{1}{l|}{\$1 million}     & \multicolumn{1}{l|}{\$1 million} &  \\ \cline{1-4}
                                &                                     &                                      &                                  &
\end{tabular}
\caption{Allais Paradox: First Choice}
\label{t6}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\cline{2-4}
\multicolumn{1}{l|}{}           & \multicolumn{1}{l|}{Tickets 1--89} & \multicolumn{1}{l|}{Tickets 90--99} & \multicolumn{1}{l|}{Ticket 100}  &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Lottery G} & \multicolumn{1}{l|}{\$0}    & \multicolumn{1}{l|}{\$5 million}     & \multicolumn{1}{l|}{\$0}         &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Lottery H} & \multicolumn{1}{l|}{\$0}    & \multicolumn{1}{l|}{\$1 million}     & \multicolumn{1}{l|}{\$1 million} &  \\ \cline{1-4}
                                &                                     &                                      &                                  &
\end{tabular}
\caption{Allais Paradox: Second Choice}
\label{t7}
\end{table}

Choosing lottery D in the first choice, and lottery G in the second choice violates independence and the sure-thing principle. To start with the sure-thing principle, note that in both choices, the two lotteries to be chosen from are identical with regard to what happens if tickets 1--89 are drawn. And thus, according to the sure-thing principle, the only thing that matters for the overall assessment should be what happens if tickets 90--100 are drawn. But for these tickets, the first choice, between lottery C and lottery D, and the second choice, between lottery G and lottery H are identical. And so, the agent should choose lottery D in the first choice if and only if she chose lottery H in the second choice. Similar reasoning applies for independence, if we regard each lottery as a compound lottery of the sub-lotteries involving tickets 1--89 and 90--100 respectively.

Nevertheless, choosing lottery D in the first choice and lottery G in the second choice is both common\footnote{See, for instance \citet{Morrison1967} for experimental evidence that many people choose this way.} and does not seem intuitively irrational. Unless some redescription strategy works to reconcile Allais preferences with expected utility theory, expected utility theory must declare these preferences irrational. Redescribing the outcomes to take account of disappointment (or regret) arguably cannot do away with the violation of separability in the Allais Paradox. \citet{Weber1998} provides an extensive argument to that effect. The Ellsberg Paradox (Section \ref{subs42}) is another case that cannot easily be dealt with by redescription. These examples suggest that there are more problematic types of interdependence in value between outcomes in different states of the world that cannot be as easily reconciled with expected utility theory as the Machina Paradox. They have consequently been an important motivation for alternatives to expected utility theory (see Section \ref{s6}).

There might, however, be good arguments in favour of the verdict that violations of separability, like the Allais preferences, are genuinely irrational. Savage himself, as well as \citet{Broome1991} argue that our reasons for choosing one act or another must depend on states of affairs where the two acts do not yield the same outcome. This seems to speak in favour of the sure-thing principle. However, as Broome acknowledges, this assumes that reasons for action themselves are separable. Somewhat more promisingly, he suggests that, if the kind of rationality we are interested in is instrumental rationality, then all our reasons for action must derive from what it would be like to have performed an action in the various states that might come about.

\citet{Buchak2013}, who, as we will see, defends an alternative to expected utility theory, argues that instrumental rationality does not require separability. In any case, note that, even if expected utility theory is right that separability is a requirement of rationality, examples like the Allais Paradox still show expected utility theory to be quite revisionary. Expected utility theory declares preferences that are common and seem intuitively reasonable as irrational. While this may not be troubling in the case of the normative and action-guiding projects, this at least seriously calls into question whether expected utility theory can serve the interpretive project.

\subsection{Separability and risk aversion}\label{subs53}

Examples like the Allais Paradox seem to show that agents actually care about some values that are not separable. The Allais preferences, for instance, make sense for an agent who cares about certainty. Lottery D in the first choice seems attractive because it leads to a gain of \$1 million for certain. If the agent does not care merely about the feeling of being certain, but instead cares about it actually being certain that she gets \$1 million, then certainty is a value that is only realised by a combination of outcomes across different states.

\citet{Buchak2013} calls agents who are sensitive to values that are only realised by a combination of outcomes across different states (other than expected utility itself) `globally sensitive'. Agents who are globally sensitive are sensitive to features other than the expected utility of an act. Next to certainty, \citet{Lopes1981, Lopes1996} argues that mean, mode, variance, skewness and probability of loss are further global features of gambles agents may care about. She argues that a normatively compelling theory of decision-making under risk would have subjects weigh off these various different criteria. \citet{Buchak2013}, too, argues that global sensitivity can be rational, under certain constraints.\footnote{There is some debate whether global sensitivity can also be made compatible with expected utility theory. \citet{Weirich1986} argues that globally sensitive aversion to risk can be represented with disutilities that are assigned to outcomes. In the context of Buchak's theory, \citet{Pettigrew2014} argues that the global sensitivity allowed for by her theory is compatible with expected utility theory if outcomes are appropriately redescribed.}

It has been argued that expected utility theory has trouble more generally in accounting for our ordinary attitudes to risk. In expected utility theory, risk averse behaviour, such as preferring a sure amount of money to a risky gamble with a higher expected monetary gain, is always explained by the concavity of the utility function with regard to the good in question. When a utility function is concave, the marginal utility derived from a good is decreasing: any additional unit of the good is worth less the more of the good the agent already has. When the utility function in money is concave in this way, the expected utility of a monetary gamble will be less than the utility of the expected monetary value. And this can mean that the agent rejects gambles that have positive expected monetary value.

Figure \ref{f2} illustrates this for an agent with utility function $u(m) = \sqrt{m}$ and current wealth of \$100, who is offered a 50/50 chance of either losing \$100 or gaining \$125. For her, the expected utility of accepting this gamble is $0.5 \cdot \sqrt{0} + 0.5 \cdot \sqrt{225} = 7.5$. This is less than the agent's current utility level of $\sqrt{100} = 10$. The agent would reject the gamble even though it leads to an expected gain of \$12.50.\footnote{See \citet{Mas-Colell1995}, chapter 6 for more detail on expected utility theory's treatment of risk aversion.}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
  axis lines=middle,
  clip=false,
  ymin=0,
  xticklabels=\empty,
  yticklabels=\empty,
  legend pos=north east,
  xlabel={$m$},
ylabel={$u(m)$},
]
\addplot+[mark=none,samples=200,unbounded coords=jump] {sqrt(x)};
\legend{$u(m)=\sqrt{m}$}
\draw[fill] (axis cs:2.25,0) circle [radius=1.5pt] node[below] {$225$};
\draw[fill] (axis cs:1,0) circle [radius=1.5pt] node[below] {$100$};
\draw[fill] (axis cs:0,0) circle [radius=1.5pt] node[below] {$0$};
\draw[fill] (axis cs:{2.25,sqrt(2.25)}) circle [radius=1.5pt] node[below right]{};
\draw[fill] (axis cs:{0,sqrt(2.25)}) circle [radius=1.5pt] node[above left] {$15$};
\draw[fill] (axis cs:{1,sqrt(1)}) circle [radius=1.5pt] node[above left]{};
\draw[fill] (axis cs:{0,sqrt(1)}) circle [radius=1.5pt] node[above left] {$10$};
\draw[fill] (axis cs:{0, 0.5*sqrt(2.25)}) circle [radius=1.5pt] node[below left] {$7.5$};
\draw[fill] (axis cs:{2.25, 0.5*sqrt(2.25)}) circle [radius=1.5pt] node[below right] {};
\draw[dashed] (axis cs:{2.25, 0.5*sqrt(2.25)}) -- (axis cs:{0, 0.5*sqrt(2.25)}) ;
\draw[dashed] (axis cs:{2.25, sqrt(2.25)}) -- (axis cs:{0, sqrt(2.25)}) ;
\draw[dashed] (axis cs:{2.25, sqrt(2.25)}) -- (axis cs:{2.25, 0}) ;
\draw[dashed] (axis cs:{1, 1}) -- (axis cs:{1, 0}) ;
\draw[dashed] (axis cs:{1, 1}) -- (axis cs:{0, 1}) ;
\end{axis}
\end{tikzpicture}
\caption{A concave utility function}
\label{f2}
\end{figure}

However, there are results suggesting that decreasing marginal utility alone cannot adequately explain ordinary risk aversion. For monetary gambles, it can be shown that according to expected utility theory, any significant risk aversion on a small scale implies implausibly high levels of risk aversion on a large scale. For instance, \citet{RabinThaler2001} show that an expected utility maximiser with an increasing, concave utility function in wealth who turns down a 50/50 bet of losing \$10 and winning \$11 will turn down any 50/50 bet involving a loss of \$100, no matter how large the potential gain. Conversely, any normal level of risk aversion for high stakes gambles implies that the agent is virtually risk neutral for small stakes gambles.\footnote{See \citet{Samuelson1963} and \citet{Rabin2000} for similar results.} These results are troubling because we are all risk averse for small stakes gambles, and we are all willing to take some risky gambles with larger stakes. Moreover, this does not seem to be intuitively irrational.

Another, more direct line of critique of the way expected utility theory deals with risk aversion is available to realists about utility. If we think of utility in the realist sense, for instance as measuring the strength of our desire, it seems like we can be risk averse with regard to goods for which our utility is not diminishing. But according to expected utility theory, we cannot be risk averse with regard to utility itself. For realists, depending on their interpretation of utility, this may be counterintuitive.\footnote{See \citet{Buchak2013} for this line of critique, as well as more examples of risk aversion that expected utility has trouble making sense of.}

\section{Alternatives to Expected Utility Theory}\label{s6}

Most alternatives to expected utility theory have been introduced as descriptive theories of choice under uncertainty, with no claim to capturing rational choice. The most well-known is prospect theory, introduced by \citet{KahnemanTversky1979}. Its most distinctive features are firstly, that it includes an editing phase, in which agents simplify their decision problems to make them more manageable, and secondly, that outcomes are evaluated as losses and gains relative to some reference point. In prospect theory, losses can be evaluated differently from gains. Since different ways of presenting a decision problem may elicit different reference points, this means that the agents described in prospect theory are sensitive to `framing'. While real agents are in fact subject to framing effects,\footnote{See, for instance, \citet{TverskyKahneman1981}.} sensitivity to framing is commonly regarded as irrational.

Alternatives to expected utility theory in the economic literature, too, have given up the idea that agents maximise a utility function that is independent of some reference point. Generalised expected utility theory, as developed in \citet{Machina1982}, for instance, introduces local utility functions, one for each lottery the agent may face. The lack of a stable utility function makes it difficult to interpret these theories as theories of instrumental rationality.

Other non-expected utility theories, in particular rank-dependent utility theory, as introduced by \citet{Quiggin1982}, use a stable utility function. In contrast to expected utility theory, however, they introduce alternative weightings of the utilities of outcomes. While in expected utility theory, an outcome's utility is weighted only by its probability, in rank-dependent utility theory, weights depend not only on the probability of an outcome, but also its rank amongst all the possible outcomes of the action. This allows the theory to model agents caring disproportionately about especially good and especially bad low probability outcomes.

\citet{Buchak2013} introduces risk-weighted expected utility theory, in which a `risk function' plays the role of the weighting function. In contrast to older rank-dependent utility theories, she argues that risk-weighted expected utility theory provides us with utilities and probabilities which can be interpreted as representing the agent's ends and beliefs respectively, and a risk function, which represents the agent's preferences over how to structure the attainment of her ends.\footnote{For an overview of other alternatives to expected utility theory in the economic literature, the two most comprehensive surveys are \citet{Schmidt2004} and \citet{Sugden2004}.}

There is a research programme in the psychological literature that studies various heuristics that agents use when making decisions in the context of uncertainty. While these are usually not intended as normative theories of rational choice, they have plausibility as action-guiding theories---theories that cognitively limited agents may use in order to approximate a perfectly rational choice. \citet{Payneetal1993}, for instance, introduce an adaptive approach to decision-making, which is driven by the tradeoff between cognitive effort and accuracy.
\citet{GigerenzerTodd2000} introduce various ``fast-and-frugal'' heuristics to decision-making under uncertainty.

\section{Dynamic Choice}\label{s7}

So far, we have looked at individual decisions separately, as one-off choices. However, each of our choices is part of a long series of choices we make in our lives. Dynamic choice theory models this explicitly. In dynamic choice problems, choices, as well as the resolution of uncertainty happen sequentially. Dynamic choice problems are typically represented as decision trees, like the one in Figure \ref{f1}. The round nodes in this tree are chance nodes, where we think of the agent as going `left' or `right' depending on what state of affairs comes about. The square nodes are decision nodes, where the agent can decide whether to go `left' or `right'.

There are a number of interesting cases where an agent ends up making a series of seemingly individually rational choices that leave her worse off than she could be.\footnote{One example is the Self-Torturer Problem discussed in Section \ref{subs43}. \citet{Andreou2012} is a helpful overview of more such cases.} Dynamic choice theory helps us analyse such cases. Here I want to focus on dynamic choice problems involving agents who violate standard expected utility theory. These cases provide some of the most powerful arguments in favour of expected utility theory, and against the alternatives canvassed in Section \ref{s6}. We already mentioned Elga's dynamic choice argument against imprecise probabilities in Section \ref{subs42}. Here, I turn to arguments involving violations of separability.

\subsection{Dynamic arguments in favour of separability}\label{subs71}

\citet{Machina1989} discusses the following dynamic version of the Allais Paradox. This dynamic version serves as an argument against Allais preferences, and violations of separability more generally. In this dynamic version, agents only get to make a decision after some of the uncertainty has already been resolved. They make a choice after they have found out whether one of tickets 1--89 has been drawn, or one of tickets 90--100 has been drawn, as shown in Figure \ref{f1}.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.52\textwidth}
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}[]
\small
\node[chance]{}
      child{node[line]{\$1 million} edge from parent
            node[above]{Tickets 1-89}}
      child{node[decision]{}
  child{node[line]{\$1 million} edge from parent
            node[above]{Lottery D}}
      child{node[chance]{}
       child{node[line]{\$0}   edge from parent
            node[above]{Ticket 100}}
 child{node[line]{\$5 million}   edge from parent
            node[above]{Tickets 90-99}}
      edge from parent
            node[below]{Lottery C}}
      edge from parent
            node[below]{Tickets 90-100}
        };
\end{tikzpicture}
        }
        \caption{First Choice}
        \label{subf11}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \resizebox{\linewidth}{!}{
    \begin{tikzpicture}[]
\small
\node[chance]{}
      child{node[line]{\$0} edge from parent
            node[above]{Tickets 1-89}}
      child{node[decision]{}
  child{node[line]{\$1 million} edge from parent
            node[above]{Lottery H}}
      child{node[chance]{}
       child{node[line]{\$0}   edge from parent
            node[above]{Ticket 100}}
 child{node[line]{\$5 million}   edge from parent
            node[above]{Tickets 90-99}}
      edge from parent
            node[below]{Lottery G}}
      edge from parent
            node[below]{Tickets 90-100}
        };
\end{tikzpicture}
}
        \caption{Second Choice}
        \label{subf12}
    \end{subfigure}
\caption{Dynamic Allais Problem}
\label{f1}
\end{figure}

The interesting feature of the dynamic case is that at the time where the agent gets to make a decision, the rest of the tree, sometimes called the `continuation tree', looks the same for the first and second choice. We might think that this means that the agent should decide the same in both cases. But then she will end up choosing in accordance either with lotteries C and G respectively, or with lotteries D and H respectively, but not according to the Allais preferences. That in turn means that for at least one of the choices, an agent with Allais preferences will end up choosing contrary to what she would have preferred at the beginning of the decision problem, before any uncertainty has been resolved.

This has been held to be problematic for a variety of reasons. Firstly, for the agent we are considering, the dynamic structure of the decision problem clearly makes a difference to what she will choose. It can make a difference whether the agent faces a one-off choice or a dynamic version of that choice involving the same possible outcomes. But, it is claimed, for instrumentally rational agents, who care only about the final outcomes, the temporal structure of a decision problem should not matter. Secondly, suppose the agent anticipates that, after uncertainty has been removed, she will go against the preferences she has at the outset. Such an agent would presumably be willing to pay to either not have uncertainty removed, or to restrict her own future choices. Paying money for this looks like a pragmatic cost of having these kinds of preferences. Moreover, refusing free information has been argued to be irrational in its own right.\footnote{See, for instance, \citet{Wakker1988}.} Thirdly, the agent does not seem to have a stable attitude towards the choice to be made in the dynamic decision problem, even though her underlying preferences over outcomes do not change. All of these considerations have been argued to count against the instrumental rationality of an agent with Allais preferences.

Similar dynamic choice problems can be formulated whenever there is a violation of separability. In Savage's framework, whenever the agent's attitudes are non-separable for two events, one can construct decision problems where the two events are de facto `separated' by revealing which of the events occurs before the agent gets to decide. And then parallel problems will arise. In fact, if we find the previous argument against Allais preferences convincing, we can formulate a very general argument in favour of expected utility theory. Spelling out the argument from consequentialism in \citet{Hammond1988} in more precise terms, \citet{McClennen1990} shows that, given some technical assumptions, expected utility theory can be derived from versions of the following principles:
\begin{description}
\item[NEC (normal-form/extensive-form coincidence):] In any dyn\-amic decision problem, the agent should choose the same as she would, were she to simply choose one course of action at the beginning of the decision problem.
\item[SEP (dynamic separability):] Within dynamic decision problems, the agent treats continuation trees as if they were new trees.
\item[DS (dynamic consistency):] The agent does not make plans she foreseeably will not execute.
\end{description}

A similar argument is made by \citet{Seidenfeld1988}. The third condition in McClennen's formulation is fairly uncontroversial. However, those defending alternatives to expected utility theory have called into question both NEC and SEP. \citet{Buchak2013} discusses both the strategy of abandoning SEP and that of abandoning NEC, and argues that at least one of them works.

SEP is characteristic of a choice strategy that was first described by \citet{Strotz1956}, and is now known in the literature as `sophisticated choice'.\footnote{See \citet{McClennen1990} for a characterisation of different dynamic choice rules.} Sophisticated agents treat continuation trees within dynamic choice problems as if they were new tress. Moreover, they anticipate, at the beginning of the dynamic choice problem, that they will do so. Given this prediction of their own future choice, they choose the action that will lead to their most preferred prospect. They thus follow a kind of `backward induction' reasoning. Sophisticated agents fail to abide by NEC: they can end up choosing courses of action that are disprefered at the beginning of the choice problem. This can be seen in our example of the dynamic Allais Paradox. Sophisticated agents behave in the way we assumed above. They thus suffer the pragmatic disadvantages we described.\footnote{In fact, Seidenfeld discusses cases where sophisticated agents end up making a sure loss.}

Those who question NEC allow that the dynamic structure of a decision problem can sometimes make a difference, even if that may have tragic consequences. But note that one can question NEC as a general principle and still think that in the particular dynamic choice problems we are considering, the pragmatic disadvantages count against having preferences that violate separability.

Because of the difficulties associated with sophistication described above, many advocates of alternatives to expected utility theory have rejected SEP instead. For instance, \citet{Machina1989} argues that SEP is close enough to separability that accepting SEP begs the question against separability. If SEP is given up, it can make a difference to an agent if she finds herself in the middle of a dynamic choice problem rather than at the beginning of a new one. One choice rule that then becomes open to her is `resolution', where the agent simply goes through with a plan she made at the beginning of a decision problem. Resolute agents obviously abide by NEC and avoid any pragmatic disadvantages. A restricted version of this dynamic choice rule is advocated by \citet{McClennen1990}.\footnote{Note that related notions of resolution are also discussed in the non-formal literature in order to deal with problems of diachronic choice, such as the Toxin Puzzle, described in \citet{Kavka1983}. See, for instance, \citet{Holton2009} and \citet{Bratman1998}, as well as the discussion on the Self-Torturer Problem in Section \ref{subs43} above.} \citet{Rabinowicz1995} argues that sophistication and resolution can be reconciled.

\subsection{Time preferences and discounting}\label{subs72}

While dynamic choice theory is concerned with the temporal sequence of our decisions, there is another branch of decision theory that is concerned with the timing of the costs and benefits that are caused by our actions. This literature studies the nature of our time preferences: do we prefer for an outcome to occur earlier or later? How much would we give up in order to receive it earlier or later?

Since most agents prefer for good outcomes to occur earlier, and bad outcomes to occur later, \citet{Samuelson1937} proposed the discounted utility model. According to this model, agents assign the same utility to an outcome (in Samuelson's model these are consumption profiles) no matter when it occurs, but discount that utility with a fixed exponential discount rate. They can then calculate how much a future outcome is worth to them at the time of decision, and maximise their discounted utility. In the case where decisions are made under certainty, let the outcomes occurring at different points in time, up until period $t$, be $O_1, ..., O_t$. The agent assigns utility $u(O)$ to each of these outcomes. This is an `instantenous' utility function, where the timing of the outcome does not matter for the utility assignment. Moreover, let $d$ be the discount factor. The agent's discounted utility $DU(O_1, ..., O_t)$ is then given by:
$$DU(O_1, ..., O_t) = \sum\limits_{i=1}^t d^i \cdot u (O_{i})$$
This discounted utility describes the current value of the stream of outcomes $O_1, ..., O_t$ to the agent. According to the discounted utility model, agents maximise this discounted utility. When we have $0 < d < 1$, the agent prefers good outcomes to occur sooner rather than later. In that case, it is also true that the value of an infinite, constant stream of benefits will be finite. \citet{Koopmans1960} presents a number of axioms on time preferences, and provides a representation theorem for the discounted utility model.

One main advantage of being the type of agent who abides by the discounted utility model is that for such an agent, there will be no preference reversals as time moves on (this feature is sometimes referred to as `time consistency'). That is, an agent will never suddenly reverse her preference between two actions as she gets closer in time to a choice. Yet, such preference reversals are common.\footnote{For empirical evidence of this phenomenon, see, for instance, \citet{Thaler1981}.} It has been argued that the hyperbolic discounting model advocated by \citet{Ainslie1992}, which allows for such reversals, models the ordinary decision-maker better. Whether the discounted utility model is normatively adequate is controversial, and depends in part on whether we think that time inconsistency is necessarily irrational.\footnote{\citet{Fredericketal2002} provide a helpful overview of this debate, and the literature on time preferences more generally.} In fact, time inconsistent preferences, just like preferences that violate expected utility theory, may lead to problematic patterns of choice in dynamic choice problems, unless the agent adopts the right dynamic choice rule.

The discounted utility model underlies much public decision-making. Discount rates are standardly applied in cost-benefit analyses. This has received special philosophical attention in the case of cost-benefit analyses of the effects of climate change. Ethicists and economists have debated whether a strictly positive discount rate is justified when evaluating the costs of climate change.\footnote{See, in particular, the debate between \citet{Stern2007} and \citet{Nordhaus2007}. For a philosopher who holds that there is no justification for time preference in public decision-making, see \citet{Broome1994}.} Much recent work on time preference and discounting has focused on how to discount in the context of uncertainty. Again, this question is especially important for evaluating the costs of climate change, since these evaluations are carried out in the context of great uncertainty. \citet{Gollier2002} provides an expected utility based model of discounting under uncertainty that much of this literature appeals to. \citet{Weitzman2009} discusses discounting in a context where our estimates of future climate have `fat tails', and argues that fat tails make a big difference to our evaluations of the costs of climate change.

\section{Concluding Remarks}

This entry started out by introducing decision theories that can be classified under the heading of `expected utility theory'. Expected utility theory is an enormously influential theory about how we do and should make choices. It has been fruitfully applied in many different fields, not least philosophy. This entry has described expected utility theory, discussed how it can be applied to the choices real agents face, and introduced debates about its foundations and interpretation.

Much recent discussion in decision theory concerns the two main types of challenge to traditional expected utility theory that the latter half of this entry focused on. The first type of challenge claims that traditional expected utility theory requires agents to have attitudes that are too fine-grained and too extensive. According to this challenge, agents have attitudes, and are rationally permitted to have attitudes that are imprecise, or vague, or incomplete. The important question arising for expected utility theory is whether it can incorporate imprecision, vagueness, and incompleteness, or whether it can instead offer a convincing argument that these attitudes are indeed irrational.

The second type of challenge questions the assumption of separability that underlies expected utility theory---that is, the assumption that the value of an outcome in one state of the world is independent of what happens in other, incompatible states of the world. According to this challenge, agents have attitudes to risky prospects that violate this assumption, and are rationally permitted to do so. This challenge, in particular, has inspired alternatives to expected utility theory. Alternatives to expected utility theory face challenges of their own, however, not least the question of whether they can make sense of dynamic choice.

\section*{Acknowledgements}

I am grateful to Seamus Bradley, Richard Pettigrew, Sergio Tenenbaum, and Jonathan Weisberg for many helpful comments on earlier drafts of this entry.